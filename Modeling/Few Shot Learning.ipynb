{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"rpJCVPcXNAsa","executionInfo":{"status":"ok","timestamp":1699895407582,"user_tz":-540,"elapsed":480,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import glob\n","from PIL import Image\n","import numpy as np\n","import cv2\n","\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","from tqdm import tqdm\n","\n","from easyfsl.samplers import TaskSampler\n","from easyfsl.utils import plot_images, sliding_average"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhZ7-7toNKIP","outputId":"c626f2cc-4135-4824-bb45-c4eed67760b7","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":2686,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xJTy9jIvNITm","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":5,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["# cv2는 이미지 색상을 BGR로 불러와서, RGB 색상으로 변환해주는 함수\n","def BGR2RGB(image):\n","    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    return rgb_img\n","\n","### Normalization ###\n","\n","# 컬러 이미지 Normalization 함수\n","def Normalization_Color(image):\n","    normalize_img = cv2.normalize(BGR2RGB(image), None, 0, 255, cv2.NORM_MINMAX) # RGB 이미지에 cv2.normalize() 적용\n","    shape = normalize_img.shape # 제대로 적용됐는지 이미지 shape 확인하기 위한 용도\n","    return normalize_img, shape\n","\n","# 그레이 이미지 Normalization 함수\n","def Normalization_Gray(image):\n","    img = cv2.cvtColor(BGR2RGB(image), cv2.COLOR_RGB2GRAY) # RGB 색상을 Gray 색상으로 변환\n","    normalize_img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX) # Gray 이미지에 cv2.normalize() 적용\n","    shape = normalize_img.shape # 제대로 적용됐는지 이미지 shape 확인하기 위한 용도\n","    return normalize_img, shape\n","\n","# 컬러 이미지 HE 함수\n","def HE_Color(image):\n","    '''\n","    컬러 이미지에 HE 적용할 땐 밝기 정보와 색상 정보를 분리한 후, 밝기 정보인 Y에 대해서만 HE 수행함\n","    색상 정보는 변환되지 않아서 색감이 그대로 유지되고, 밝기 정보인 Y에 대해서만 명암비가 증가하게 됨\n","    '''\n","    img = cv2.cvtColor(BGR2RGB(image), cv2.COLOR_RGB2YCrCb) # RGB 색상을 YCrCb 색상으로 변환\n","    img_planes = cv2.split(img) # Y(밝기 정보), Cr & Cb(색상 정보)로 split\n","    img_planes_0 = cv2.equalizeHist(img_planes[0]) # 밝기 정보인 Y에 대해서만 cv2.equalizeHist() 적용\n","    merge_img = cv2.merge([img_planes_0, img_planes[1], img_planes[2]]) # 변환된 Y와 색상 정보 merge\n","    he_img = cv2.cvtColor(merge_img, cv2.COLOR_YCrCb2RGB) # YCrCb 색상을 RGB 색상으로 변환\n","    shape = he_img.shape # 제대로 적용됐는지 이미지 shape 확인하기 위한 용도\n","    return he_img, shape\n","\n","# 그레이 이미지 HE 함수\n","def HE_Gray(image):\n","    img = cv2.cvtColor(BGR2RGB(image), cv2.COLOR_RGB2GRAY) # RGB 색상을 Gray 색상으로 변환\n","    he_img = cv2.equalizeHist(img) # Gray 이미지에 cv2.equalizeHist() 적용\n","    shape = he_img.shape # 제대로 적용됐는지 이미지 shape 확인하기 위한 용도\n","    return he_img, shape"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"I11Y07Y6tqi_","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":5,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["# grayscale None\n","# class CustomTransform:\n","#     def __call__(self, img):\n","#         img = np.array(img)\n","#         img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","#         #img, _ = HE_Gray(img)\n","#         #img, _ = Normalization_Gray(img)\n","#         return Image.fromarray(img)\n","\n","# grayscale HE\n","# class CustomTransform:\n","#     def __call__(self, img):\n","#         img = np.array(img)\n","#         #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","#         img, _ = HE_Gray(img)\n","#         #img, _ = Normalization_Gray(img)\n","#         return Image.fromarray(img)\n","\n","# grayscale Norm\n","class CustomTransform:\n","    def __call__(self, img):\n","        img = np.array(img)\n","        #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        #img, _ = HE_Gray(img)\n","        img, _ = Normalization_Gray(img)\n","        return Image.fromarray(img)\n","\n","# grayscale Norm&HE\n","# class CustomTransform:\n","#     def __call__(self, img):\n","#         img = np.array(img)\n","#         #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","#         img, _ = HE_Gray(img)\n","#         img, _ = Normalization_Gray(img)\n","#         return Image.fromarray(img)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"xnvnuGzMNAsd","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":4,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["N_WAY = 2\n","N_SHOT = 2\n","N_QUERY = 2\n","N_EVALUATION_TASK = 100"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"TmXz4REANAse","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":4,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["import os\n","import glob\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, base_dir, transform=None):\n","        normal_img_paths = glob.glob(os.path.join(base_dir, '0', '*.jpg'))\n","        abnormal_img_paths = glob.glob(os.path.join(base_dir, '1', '*.jpg'))\n","\n","        self.img_paths = normal_img_paths + abnormal_img_paths\n","        self.transform = transform\n","\n","        self.img_labels = [int(os.path.split(os.path.dirname(path))[-1]) for path in self.img_paths]\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        image = Image.open(img_path)\n","        label = self.img_labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"52U2hzfKNAse","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":4,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["train_set = CustomDataset(\n","    base_dir='/content/drive/MyDrive/data_all (1)/data_all/train_data',\n","    transform=transforms.Compose(\n","        [\n","            CustomTransform(),\n","            transforms.Resize((512, 512)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","        ]\n","    )\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"jFpMXa8_NAse","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":4,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["N_TRAINING_EPISODES = 10000\n","N_VALIDATION_TASK = 100\n","\n","train_set.get_labels = lambda: train_set.img_labels\n","train_sampler = TaskSampler(\n","    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",")\n","\n","train_loader = DataLoader(\n","    train_set,\n","    batch_sampler = train_sampler,\n","    num_workers = 1,\n","    pin_memory = True,\n","    collate_fn = train_sampler.episodic_collate_fn,\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4s6ikBkNAsf","outputId":"c4cd25b3-3e5c-44c8-a149-1ed001888410","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":4,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}],"source":["class PrototypicalNetworks(nn.Module):\n","    def __init__(self, backbone: nn.Module):\n","        super(PrototypicalNetworks, self).__init__()\n","        self.backbone = backbone\n","\n","    def forward(\n","        self,\n","        support_images: torch.Tensor,\n","        support_labels: torch.Tensor,\n","        query_images: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Predict query labels using labeled support images.\n","        \"\"\"\n","        # Extract the features of support and query images\n","        z_support = self.backbone.forward(support_images)\n","        z_query = self.backbone.forward(query_images)\n","\n","        # Infer the number of different classes from the labels of the support set\n","        n_way = len(torch.unique(support_labels))\n","        # Prototype i is the mean of all instances of features corresponding to labels == i\n","        z_proto = torch.cat(\n","            [\n","                z_support[torch.nonzero(support_labels == label)].mean(0)\n","                for label in range(n_way)\n","            ]\n","        )\n","\n","        # Compute the euclidean distance from queries to prototypes\n","        dists = torch.cdist(z_query, z_proto)\n","\n","        # And here is the super complicated operation to transform those distances into classification scores!\n","        scores = -dists\n","        return scores\n","\n","\n","convolutional_network = resnet18(pretrained=True)\n","# convolutional_network.fc = nn.Flatten()\n","num_ftrs = convolutional_network.fc.in_features\n","convolutional_network.fc = nn.Linear(num_ftrs, 2)\n","\n","convolutional_network.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","\n","print(convolutional_network)\n","\n","model = PrototypicalNetworks(convolutional_network).cuda()"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ZoVCjHfMNAsf","executionInfo":{"status":"ok","timestamp":1699895410885,"user_tz":-540,"elapsed":3,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def fit(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n","    query_labels: torch.Tensor,\n",") -> float:\n","    optimizer.zero_grad()\n","    classification_scores = model(\n","        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n","    )\n","\n","    loss = criterion(classification_scores, query_labels.cuda())\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ls-E2r6dNAsf","outputId":"6f185487-345a-4db6-81bf-5977c8026201","executionInfo":{"status":"ok","timestamp":1699897858850,"user_tz":-540,"elapsed":2447968,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [40:47<00:00,  4.09it/s, loss=0]\n"]}],"source":["log_update_frequency = 10\n","\n","all_loss = []\n","model.train()\n","with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n","    for episode_index, (\n","        support_images,\n","        support_labels,\n","        query_images,\n","        query_labels,\n","        _,\n","    ) in tqdm_train:\n","        loss_value = fit(support_images, support_labels, query_images, query_labels)\n","        all_loss.append(loss_value)\n","\n","        if episode_index % log_update_frequency == 0:\n","            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ZC99rtzrNAsg","executionInfo":{"status":"ok","timestamp":1699897859683,"user_tz":-540,"elapsed":845,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["test_set = CustomDataset(\n","    base_dir='/content/drive/MyDrive/data_all (1)/data_all/test_data',\n","    transform=transforms.Compose(\n","        [\n","            CustomTransform(),\n","            transforms.Resize((512, 512)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","        ]\n","    )\n",")\n","\n","N_EVALUATION_TASKS = 100\n","\n","test_set.get_labels = lambda: test_set.img_labels\n","test_sampler = TaskSampler(\n","    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",")\n","\n","test_loader = DataLoader(\n","    test_set,\n","    batch_sampler=test_sampler,\n","    num_workers=1,\n","    pin_memory=True,\n","    collate_fn=test_sampler.episodic_collate_fn,\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awbImy2zNAsg","outputId":"aba11b08-4986-4d37-db52-bcf25c5dc435","executionInfo":{"status":"ok","timestamp":1699897890923,"user_tz":-540,"elapsed":31244,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:31<00:00,  3.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Model tested on 100 tasks. Accuracy: 99.25%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["def evaluate_on_one_task(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n","    query_labels: torch.Tensor,\n",") -> [int, int]:\n","    \"\"\"\n","    Returns the number of correct predictions of query labels, and the total number of predictions.\n","    \"\"\"\n","    return (\n","        torch.max(\n","            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n","            .detach()\n","            .data,\n","            1,\n","        )[1]\n","        == query_labels.cuda()\n","    ).sum().item(), len(query_labels)\n","\n","\n","def evaluate(data_loader: DataLoader):\n","    total_predictions = 0\n","    correct_predictions = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for episode_index, (\n","            support_images,\n","            support_labels,\n","            query_images,\n","            query_labels,\n","            class_ids,\n","        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n","\n","            correct, total = evaluate_on_one_task(\n","                support_images, support_labels, query_images, query_labels\n","            )\n","\n","            total_predictions += total\n","            correct_predictions += correct\n","\n","    print(\n","        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n","    )\n","\n","\n","evaluate(test_loader)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"-yos1M2eNAsg","executionInfo":{"status":"ok","timestamp":1699897912500,"user_tz":-540,"elapsed":572,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["torch.save(model.state_dict(), '/content/drive/MyDrive/data_all (1)/Few Shot Learning_1_512_Normalization.pth')"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"VsuwECuQNAsh","executionInfo":{"status":"ok","timestamp":1699897913115,"user_tz":-540,"elapsed":2,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["# import os\n","# import glob\n","\n","# import torch\n","# import torch.nn as nn\n","# from torchvision.models import resnet18\n","# from torchvision import transforms\n","# from torch.utils.data import Dataset, DataLoader\n","# from PIL import Image\n","\n","\n","\n","# class PrototypicalNetworks(nn.Module):\n","#     def __init__(self, backbone: nn.Module):\n","#         super(PrototypicalNetworks, self).__init__()\n","#         self.backbone = backbone\n","\n","#     def forward(\n","#         self,\n","#         support_images: torch.Tensor,\n","#         support_labels: torch.Tensor,\n","#         query_images: torch.Tensor,\n","#     ) -> torch.Tensor:\n","#         \"\"\"\n","#         Predict query labels using labeled support images.\n","#         \"\"\"\n","#         # Extract the features of support and query images\n","#         z_support = self.backbone.forward(support_images)\n","#         z_query = self.backbone.forward(query_images)\n","\n","#         # Infer the number of different classes from the labels of the support set\n","#         n_way = len(torch.unique(support_labels))\n","#         # Prototype i is the mean of all instances of features corresponding to labels == i\n","#         z_proto = torch.cat(\n","#             [\n","#                 z_support[torch.nonzero(support_labels == label)].mean(0)\n","#                 for label in range(n_way)\n","#             ]\n","#         )\n","\n","#         # Compute the euclidean distance from queries to prototypes\n","#         dists = torch.cdist(z_query, z_proto)\n","\n","#         # And here is the super complicated operation to transform those distances into classification scores!\n","#         scores = -dists\n","#         return scores\n","\n","\n","# convolutional_network = resnet18(pretrained=True)\n","# # convolutional_network.fc = nn.Flatten()\n","# num_ftrs = convolutional_network.fc.in_features\n","# convolutional_network.fc = nn.Linear(num_ftrs, 2)\n","\n","# print(convolutional_network)\n","\n","# model = PrototypicalNetworks(convolutional_network).cuda()\n","# model.load_state_dict(torch.load('/BTS2023/byeonjun/BTS2023/few_shot/proto/result_pth/few_shot3.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqppMaIQNAsh","executionInfo":{"status":"aborted","timestamp":1699897890924,"user_tz":-540,"elapsed":6,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["# class SupportDataset(Dataset):\n","#     def __init__(self, base_dir, transform=None):\n","#         self.normal_img_paths = glob.glob(os.path.join(base_dir, '0', '*.jpg'))\n","#         self.abnormal_img_paths = glob.glob(os.path.join(base_dir, '1', '*.jpg'))\n","\n","#         self.img_paths = self.normal_img_paths + self.abnormal_img_paths\n","#         self.img_labels = [0]*len(self.normal_img_paths) + [1]*len(self.abnormal_img_paths)\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.img_paths)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = self.img_paths[idx]\n","#         image = Image.open(img_path)\n","#         label = self.img_labels[idx]\n","\n","#         if self.transform:\n","#             image = self.transform(image)\n","\n","#         return image, label\n","\n","# class QueryDataset(Dataset):\n","#     def __init__(self, base_dir, transform=None):\n","#         self.img_paths = glob.glob(os.path.join(base_dir, '*.jpg'))\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.img_paths)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = self.img_paths[idx]\n","#         image_name = os.path.basename(img_path)\n","#         image = Image.open(img_path)\n","\n","#         if self.transform:\n","#             image = self.transform(image)\n","\n","#         return image, image_name\n","\n","\n","# from collections import Counter\n","\n","# def predict_with_support(support_loader, query_loader):\n","#     all_predictions = []\n","\n","#     for support_images, support_labels in support_loader:\n","#         support_images = support_images.cuda()\n","#         support_labels = support_labels.cuda()\n","\n","#         with torch.no_grad():\n","#             for query_images, image_names in query_loader:\n","#                 temp_preds = []\n","\n","#                 # 각 이미지에 대한 예측을 5번 수행\n","#                 for _ in range(5):\n","#                     outputs = model(support_images, support_labels, query_images.cuda())\n","#                     _, preds = torch.max(outputs, 1)\n","#                     temp_preds.append(preds)\n","\n","#                 # 3번의 예측 중 가장 많이 예측된 값을 최종 예측값으로 선택\n","#                 for i, img_name in enumerate(image_names):\n","#                     counter = Counter([temp[i].item() for temp in temp_preds])\n","#                     most_common_pred = counter.most_common(1)[0][0]\n","#                     all_predictions.append((img_name, most_common_pred))\n","\n","#     return all_predictions\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vap-b4ONAsh","executionInfo":{"status":"aborted","timestamp":1699897890924,"user_tz":-540,"elapsed":6,"user":{"displayName":"박시현","userId":"04549412973847763391"}}},"outputs":[],"source":["# transform = transforms.Compose([\n","#     transforms.Resize((256, 256)),\n","#     transforms.RandomHorizontalFlip(),\n","#     transforms.RandomVerticalFlip(),\n","#     transforms.ToTensor(),\n","# ])\n","\n","# support_set = SupportDataset(base_dir='/BTS2023/byeonjun/BTS2023/few_shot/proto/train_data', transform=transform)\n","# query_set = QueryDataset(base_dir='/BTS2023/byeonjun/BTS2023/few_shot/proto/test_all', transform=transform)\n","\n","# support_loader = DataLoader(support_set, batch_size=8, shuffle=True)\n","# query_loader = DataLoader(query_set, batch_size=8, shuffle=False)\n","\n","# predictions = predict_with_support(support_loader, query_loader)\n","# for file_path, pred in predictions:\n","#     print(f\"File: {file_path}, Prediction: {pred}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}